[[36m2024-01-09 01:11:42,344[0m][[35mHYDRA[0m] Launching 5 jobs locally[0m
[[36m2024-01-09 01:11:42,345[0m][[35mHYDRA[0m] 	#0 : seed=0 experiment=diginetica_10/gcn[0m
[[36m2024-01-09 01:11:42,663[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - [rank: 0] Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2024-01-09 01:11:42,669[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - [rank: 0] Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.mixed_graph_datamodule.MixedGraphDatamodule          
â”‚       dataset:                                                                
â”‚         _target_: src.data.components.diginetica_dataset.DigineticaDataset    
â”‚         raw_dir: /home/snow/projects/graph-learning/data//Diginetica/processed
â”‚         num_mixture: 10                                                       
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 1400                                                                  
â”‚       - 400                                                                   
â”‚       - 200                                                                   
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.graph_conv_module.GraphConvModule                  
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.encoder.GCNEncoder                    
â”‚         dim_in: 300                                                           
â”‚         dim_hidden: 300                                                       
â”‚         dim_out: 300                                                          
â”‚         num_layers: 3                                                         
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.decoder.NodeMLPDecoder                
â”‚         dim_in: 300                                                           
â”‚         dim_hidden: 300                                                       
â”‚       mapper:                                                                 
â”‚         _target_: src.models.components.mapper.MLPMapper                      
â”‚         dim_in: 300                                                           
â”‚         num_mixture: 10                                                       
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.001                                                             
â”‚         weight_decay: 1.0e-05                                                 
â”‚       num_mixture: 10                                                         
â”‚       scheduler: null                                                         
â”‚       compile: false                                                          
â”‚       learnable_feat: true                                                    
â”‚       num_features: 19519                                                     
â”‚       dim_feature: 300                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/snow/projects/graph-learning/logs/train/multiruns/2024-
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/f1                                                       
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: lightning.pytorch.callbacks.EarlyStopping                   
â”‚         monitor: val/f1                                                       
â”‚         min_delta: 0.0                                                        
â”‚         patience: 10                                                          
â”‚         verbose: false                                                        
â”‚         mode: max                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: lightning.pytorch.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: lightning.pytorch.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: lightning.pytorch.loggers.wandb.WandbLogger                 
â”‚         save_dir: /home/snow/projects/graph-learning/logs/train/multiruns/2024
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: Graph-Learning-Diginetica                                  
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags:                                                                 
â”‚         - 10-Mixtures                                                         
â”‚         - GCN                                                                 
â”‚         job_type: ''                                                          
â”‚         name: GCN                                                             
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: lightning.pytorch.trainer.Trainer                             
â”‚       default_root_dir: /home/snow/projects/graph-learning/logs/train/multirun
â”‚       min_epochs: 10                                                          
â”‚       max_epochs: 300                                                         
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚       gradient_clip_val: 0                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/snow/projects/graph-learning                            
â”‚       data_dir: /home/snow/projects/graph-learning/data/                      
â”‚       log_dir: /home/snow/projects/graph-learning/logs/                       
â”‚       output_dir: /home/snow/projects/graph-learning/logs/train/multiruns/2024
â”‚       work_dir: /home/snow/projects/graph-learning/src                        
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['10-Mixtures', 'GCN']                                                  
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â””â”€â”€ seed
    â””â”€â”€ 0                                                                       
[[36m2024-01-09 01:11:42,732[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating datamodule <src.data.mixed_graph_datamodule.MixedGraphDatamodule>[0m
[[36m2024-01-09 01:11:48,447[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating model <src.models.graph_conv_module.GraphConvModule>[0m
[[36m2024-01-09 01:11:48,506[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpgop_vr8g[0m
[[36m2024-01-09 01:11:48,507[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpgop_vr8g/_remote_module_non_scriptable.py[0m
[[36m2024-01-09 01:11:48,573[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating callbacks...[0m
[[36m2024-01-09 01:11:48,573[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2024-01-09 01:11:48,577[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2024-01-09 01:11:48,578[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2024-01-09 01:11:48,579[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2024-01-09 01:11:48,579[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating loggers...[0m
[[36m2024-01-09 01:11:48,580[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>[0m
[[36m2024-01-09 01:11:48,633[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2024-01-09 01:11:48,754[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Logging hyperparameters![0m
wandb: Currently logged in as: jfluidity. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/0/wandb/run-20240109_011150-ckjj41ev
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run GCN
wandb: â­ï¸ View project at http://192.168.1.130:37107/jfluidity/Graph%20Learning%20%28Diginetica%29
wandb: ğŸš€ View run at http://192.168.1.130:37107/jfluidity/Graph%20Learning%20%28Diginetica%29/runs/ckjj41ev
[[36m2024-01-09 01:11:53,503[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Starting training![0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name             â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder          â”‚ GCNEncoder        â”‚  272 K â”‚
â”‚ 1  â”‚ encoder.conv_in  â”‚ GraphConv         â”‚ 90.3 K â”‚
â”‚ 2  â”‚ encoder.bn_in    â”‚ BatchNorm1d       â”‚    600 â”‚
â”‚ 3  â”‚ encoder.conv_out â”‚ GraphConv         â”‚ 90.3 K â”‚
â”‚ 4  â”‚ encoder.bn_out   â”‚ BatchNorm1d       â”‚    600 â”‚
â”‚ 5  â”‚ encoder.convs    â”‚ ModuleList        â”‚ 90.3 K â”‚
â”‚ 6  â”‚ encoder.convs.0  â”‚ GraphConv         â”‚ 90.3 K â”‚
â”‚ 7  â”‚ encoder.bns      â”‚ ModuleList        â”‚    600 â”‚
â”‚ 8  â”‚ encoder.bns.0    â”‚ BatchNorm1d       â”‚    600 â”‚
â”‚ 9  â”‚ encoder.relu     â”‚ ReLU              â”‚      0 â”‚
â”‚ 10 â”‚ decoder          â”‚ NodeMLPDecoder    â”‚  180 K â”‚
â”‚ 11 â”‚ decoder.linear1  â”‚ Linear            â”‚  180 K â”‚
â”‚ 12 â”‚ decoder.linear2  â”‚ Linear            â”‚    301 â”‚
â”‚ 13 â”‚ decoder.relu     â”‚ ReLU              â”‚      0 â”‚
â”‚ 14 â”‚ mapper           â”‚ MLPMapper         â”‚  5.0 M â”‚
â”‚ 15 â”‚ mapper.fc1       â”‚ Linear            â”‚  451 K â”‚
â”‚ 16 â”‚ mapper.fc2       â”‚ Linear            â”‚  4.5 M â”‚
â”‚ 17 â”‚ mapper.relu      â”‚ ReLU              â”‚      0 â”‚
â”‚ 18 â”‚ train_precision  â”‚ BinaryPrecision   â”‚      0 â”‚
â”‚ 19 â”‚ val_precision    â”‚ BinaryPrecision   â”‚      0 â”‚
â”‚ 20 â”‚ test_precision   â”‚ BinaryPrecision   â”‚      0 â”‚
â”‚ 21 â”‚ train_recall     â”‚ BinaryRecall      â”‚      0 â”‚
â”‚ 22 â”‚ val_recall       â”‚ BinaryRecall      â”‚      0 â”‚
â”‚ 23 â”‚ test_recall      â”‚ BinaryRecall      â”‚      0 â”‚
â”‚ 24 â”‚ train_f1         â”‚ BinaryF1Score     â”‚      0 â”‚
â”‚ 25 â”‚ val_f1           â”‚ BinaryF1Score     â”‚      0 â”‚
â”‚ 26 â”‚ test_f1          â”‚ BinaryF1Score     â”‚      0 â”‚
â”‚ 27 â”‚ train_loss       â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 28 â”‚ val_f1_best      â”‚ MaxMetric         â”‚      0 â”‚
â”‚ 29 â”‚ criterion        â”‚ BCEWithLogitsLoss â”‚      0 â”‚
â”‚ 30 â”‚ sigmoid          â”‚ Sigmoid           â”‚      0 â”‚
â”‚ 31 â”‚ feat             â”‚ Embedding         â”‚  5.9 M â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.3 M                                                        
Non-trainable params: 0                                                         
Total params: 11.3 M                                                            
Total estimated model params size (MB): 45                                      
/home/snow/anaconda3/envs/graph-learning/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
/home/snow/anaconda3/envs/graph-learning/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
/home/snow/anaconda3/envs/graph-learning/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Epoch 12/299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 22/22 0:01:02 â€¢ 0:00:00 0.36it/s v_num: 41ev      
                                                               train/loss_step: 
                                                               0.018            
                                                               val/precision:   
                                                               0.370 val/recall:
                                                               1.000 val/f1:    
                                                               0.540            
                                                               train/loss_epoch:
                                                               0.016            
[[36m2024-01-09 01:26:17,494[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Starting testing![0m
Restoring states from the checkpoint path at /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/0/checkpoints/epoch_002.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
Loaded model weights from the checkpoint at /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/0/checkpoints/epoch_002.ckpt
/home/snow/anaconda3/envs/graph-learning/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚          test/f1          â”‚    0.5432988405227661     â”‚
â”‚      test/precision       â”‚    0.3729652166366577     â”‚
â”‚        test/recall        â”‚            1.0            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 4/4 0:00:05 â€¢ 0:00:00 0.87it/s 
[[36m2024-01-09 01:26:30,038[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Best ckpt path: /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/0/checkpoints/epoch_002.ckpt[0m
[[36m2024-01-09 01:26:30,042[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - [rank: 0] Output dir: /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/0[0m
[[36m2024-01-09 01:26:30,043[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - [rank: 0] Closing wandb![0m
wandb: - 0.012 MB of 0.012 MB uploaded
wandb: \ 0.035 MB of 0.052 MB uploaded (0.006 MB deduped)
wandb: | 0.035 MB of 0.052 MB uploaded (0.006 MB deduped)
wandb: / 0.052 MB of 0.052 MB uploaded (0.006 MB deduped)
wandb:                                                                                
wandb: W&B sync reduced upload amount by 12.2%             
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb:             test/f1 â–
wandb:      test/precision â–
wandb:         test/recall â–
wandb:    train/loss_epoch â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–
wandb:     train/loss_step â–…â–ˆâ–†â–„â–
wandb: trainer/global_step â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:              val/f1 â–â–†â–ˆâ–…â–ˆâ–…â–…â–…â–…â–…â–…â–†â–†
wandb:       val/precision â–â–†â–ˆâ–„â–ˆâ–„â–„â–„â–„â–„â–ƒâ–…â–…
wandb:          val/recall â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 13
wandb:             test/f1 0.5433
wandb:      test/precision 0.37297
wandb:         test/recall 1.0
wandb:    train/loss_epoch 0.01646
wandb:     train/loss_step 0.0122
wandb: trainer/global_step 286
wandb:              val/f1 0.53966
wandb:       val/precision 0.36954
wandb:          val/recall 1.0
wandb: 
wandb: ğŸš€ View run GCN at: http://192.168.1.130:37107/jfluidity/Graph%20Learning%20%28Diginetica%29/runs/ckjj41ev
wandb: ï¸âš¡ View job at http://192.168.1.130:37107/jfluidity/Graph%20Learning%20%28Diginetica%29/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjc=/version_details/v2
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/0/wandb/run-20240109_011150-ckjj41ev/logs
[[36m2024-01-09 01:26:36,209[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - [rank: 0] Metric name is None! Skipping metric value retrieval...[0m
[[36m2024-01-09 01:26:36,212[0m][[35mHYDRA[0m] 	#1 : seed=1 experiment=diginetica_10/gcn[0m
[[36m2024-01-09 01:26:36,488[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - [rank: 0] Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2024-01-09 01:26:36,491[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - [rank: 0] Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.mixed_graph_datamodule.MixedGraphDatamodule          
â”‚       dataset:                                                                
â”‚         _target_: src.data.components.diginetica_dataset.DigineticaDataset    
â”‚         raw_dir: /home/snow/projects/graph-learning/data//Diginetica/processed
â”‚         num_mixture: 10                                                       
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 1400                                                                  
â”‚       - 400                                                                   
â”‚       - 200                                                                   
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.graph_conv_module.GraphConvModule                  
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.encoder.GCNEncoder                    
â”‚         dim_in: 300                                                           
â”‚         dim_hidden: 300                                                       
â”‚         dim_out: 300                                                          
â”‚         num_layers: 3                                                         
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.decoder.NodeMLPDecoder                
â”‚         dim_in: 300                                                           
â”‚         dim_hidden: 300                                                       
â”‚       mapper:                                                                 
â”‚         _target_: src.models.components.mapper.MLPMapper                      
â”‚         dim_in: 300                                                           
â”‚         num_mixture: 10                                                       
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.001                                                             
â”‚         weight_decay: 1.0e-05                                                 
â”‚       num_mixture: 10                                                         
â”‚       scheduler: null                                                         
â”‚       compile: false                                                          
â”‚       learnable_feat: true                                                    
â”‚       num_features: 19519                                                     
â”‚       dim_feature: 300                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/snow/projects/graph-learning/logs/train/multiruns/2024-
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/f1                                                       
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: lightning.pytorch.callbacks.EarlyStopping                   
â”‚         monitor: val/f1                                                       
â”‚         min_delta: 0.0                                                        
â”‚         patience: 10                                                          
â”‚         verbose: false                                                        
â”‚         mode: max                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: lightning.pytorch.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: lightning.pytorch.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: lightning.pytorch.loggers.wandb.WandbLogger                 
â”‚         save_dir:Graph-Learning-Digineticalearning/logs/train/multiruns/2024
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: Graph Learning (Diginetica)                                  
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags:                                                                 
â”‚         - 10-Mixtures                                                         
â”‚         - GCN                                                                 
â”‚         job_type: ''                                                          
â”‚         name: GCN                                                             
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: lightning.pytorch.trainer.Trainer                             
â”‚       default_root_dir: /home/snow/projects/graph-learning/logs/train/multirun
â”‚       min_epochs: 10                                                          
â”‚       max_epochs: 300                                                         
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚       gradient_clip_val: 0                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/snow/projects/graph-learning                            
â”‚       data_dir: /home/snow/projects/graph-learning/data/                      
â”‚       log_dir: /home/snow/projects/graph-learning/logs/                       
â”‚       output_dir: /home/snow/projects/graph-learning/logs/train/multiruns/2024
â”‚       work_dir: /home/snow/projects/graph-learning/src                        
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['10-Mixtures', 'GCN']                                                  
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â””â”€â”€ seed
    â””â”€â”€ 1                                                                       
Seed set to 1
[[36m2024-01-09 01:26:36,557[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating datamodule <src.data.mixed_graph_datamodule.MixedGraphDatamodule>[0m
[[36m2024-01-09 01:26:43,008[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating model <src.models.graph_conv_module.GraphConvModule>[0m
[[36m2024-01-09 01:26:43,117[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating callbacks...[0m
[[36m2024-01-09 01:26:43,117[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2024-01-09 01:26:43,121[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2024-01-09 01:26:43,122[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2024-01-09 01:26:43,123[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2024-01-09 01:26:43,124[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating loggers...[0m
[[36m2024-01-09 01:26:43,124[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>[0m
[[36m2024-01-09 01:26:43,127[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2024-01-09 01:26:43,145[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Logging hyperparameters![0m
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/1/wandb/run-20240109_012643-31kxe86m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run GCN
wandb: â­ï¸ View project at http://192.168.1.130:37107/jfluidity/Graph%20Learning%20%28Diginetica%29
wandb: ğŸš€ View run at http://192.168.1.130:37107/jfluidity/Graph%20Learning%20%28Diginetica%29/runs/31kxe86m
[[36m2024-01-09 01:26:46,908[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Starting training![0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name             â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder          â”‚ GCNEncoder        â”‚  272 K â”‚
â”‚ 1  â”‚ encoder.conv_in  â”‚ GraphConv         â”‚ 90.3 K â”‚
â”‚ 2  â”‚ encoder.bn_in    â”‚ BatchNorm1d       â”‚    600 â”‚
â”‚ 3  â”‚ encoder.conv_out â”‚ GraphConv         â”‚ 90.3 K â”‚
â”‚ 4  â”‚ encoder.bn_out   â”‚ BatchNorm1d       â”‚    600 â”‚
â”‚ 5  â”‚ encoder.convs    â”‚ ModuleList        â”‚ 90.3 K â”‚
â”‚ 6  â”‚ encoder.convs.0  â”‚ GraphConv         â”‚ 90.3 K â”‚
â”‚ 7  â”‚ encoder.bns      â”‚ ModuleList        â”‚    600 â”‚
â”‚ 8  â”‚ encoder.bns.0    â”‚ BatchNorm1d       â”‚    600 â”‚
â”‚ 9  â”‚ encoder.relu     â”‚ ReLU              â”‚      0 â”‚
â”‚ 10 â”‚ decoder          â”‚ NodeMLPDecoder    â”‚  180 K â”‚
â”‚ 11 â”‚ decoder.linear1  â”‚ Linear            â”‚  180 K â”‚
â”‚ 12 â”‚ decoder.linear2  â”‚ Linear            â”‚    301 â”‚
â”‚ 13 â”‚ decoder.relu     â”‚ ReLU              â”‚      0 â”‚
â”‚ 14 â”‚ mapper           â”‚ MLPMapper         â”‚  5.0 M â”‚
â”‚ 15 â”‚ mapper.fc1       â”‚ Linear            â”‚  451 K â”‚
â”‚ 16 â”‚ mapper.fc2       â”‚ Linear            â”‚  4.5 M â”‚
â”‚ 17 â”‚ mapper.relu      â”‚ ReLU              â”‚      0 â”‚
â”‚ 18 â”‚ train_precision  â”‚ BinaryPrecision   â”‚      0 â”‚
â”‚ 19 â”‚ val_precision    â”‚ BinaryPrecision   â”‚      0 â”‚
â”‚ 20 â”‚ test_precision   â”‚ BinaryPrecision   â”‚      0 â”‚
â”‚ 21 â”‚ train_recall     â”‚ BinaryRecall      â”‚      0 â”‚
â”‚ 22 â”‚ val_recall       â”‚ BinaryRecall      â”‚      0 â”‚
â”‚ 23 â”‚ test_recall      â”‚ BinaryRecall      â”‚      0 â”‚
â”‚ 24 â”‚ train_f1         â”‚ BinaryF1Score     â”‚      0 â”‚
â”‚ 25 â”‚ val_f1           â”‚ BinaryF1Score     â”‚      0 â”‚
â”‚ 26 â”‚ test_f1          â”‚ BinaryF1Score     â”‚      0 â”‚
â”‚ 27 â”‚ train_loss       â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 28 â”‚ val_f1_best      â”‚ MaxMetric         â”‚      0 â”‚
â”‚ 29 â”‚ criterion        â”‚ BCEWithLogitsLoss â”‚      0 â”‚
â”‚ 30 â”‚ sigmoid          â”‚ Sigmoid           â”‚      0 â”‚
â”‚ 31 â”‚ feat             â”‚ Embedding         â”‚  5.9 M â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.3 M                                                        
Non-trainable params: 0                                                         
Total params: 11.3 M                                                            
Total estimated model params size (MB): 45                                      
/home/snow/anaconda3/envs/graph-learning/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
/home/snow/anaconda3/envs/graph-learning/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
/home/snow/anaconda3/envs/graph-learning/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Epoch 67/299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 22/22 0:00:43 â€¢ 0:00:00 0.51it/s v_num: e86m      
                                                               train/loss_step: 
                                                               0.016            
                                                               val/precision:   
                                                               0.431 val/recall:
                                                               1.000 val/f1:    
                                                               0.603            
                                                               train/loss_epoch:
                                                               0.015            
[[36m2024-01-09 02:44:37,372[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Starting testing![0m
Restoring states from the checkpoint path at /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/1/checkpoints/epoch_057.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
Loaded model weights from the checkpoint at /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/1/checkpoints/epoch_057.ckpt
/home/snow/anaconda3/envs/graph-learning/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚          test/f1          â”‚    0.6169412732124329     â”‚
â”‚      test/precision       â”‚    0.4460701644420624     â”‚
â”‚        test/recall        â”‚            1.0            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 4/4 0:00:03 â€¢ 0:00:00 1.29it/s 
[[36m2024-01-09 02:44:47,805[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Best ckpt path: /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/1/checkpoints/epoch_057.ckpt[0m
[[36m2024-01-09 02:44:47,807[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - [rank: 0] Output dir: /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/1[0m
[[36m2024-01-09 02:44:47,807[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - [rank: 0] Closing wandb![0m
wandb: - 0.026 MB of 0.026 MB uploaded
wandb: \ 0.026 MB of 0.026 MB uploaded
wandb: | 0.079 MB of 0.079 MB uploaded (0.019 MB deduped)
wandb:                                                                                
wandb: W&B sync reduced upload amount by 23.8%             
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:             test/f1 â–
wandb:      test/precision â–
wandb:         test/recall â–
wandb:    train/loss_epoch â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     train/loss_step â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ˆâ–‚â–‚â–‚â–‚â–â–‚â–‚â–â–ƒâ–â–‚â–‚â–‚
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:              val/f1 â–…â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–â–†â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:       val/precision â–„â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–…â–†â–†â–…â–â–†â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:          val/recall â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 68
wandb:             test/f1 0.61694
wandb:      test/precision 0.44607
wandb:         test/recall 1.0
wandb:    train/loss_epoch 0.01545
wandb:     train/loss_step 0.01547
wandb: trainer/global_step 1496
wandb:              val/f1 0.60284
wandb:       val/precision 0.43147
wandb:          val/recall 1.0
wandb: 
wandb: ğŸš€ View run GCN at: http://192.168.1.130:37107/jfluidity/Graph%20Learning%20%28Diginetica%29/runs/31kxe86m
wandb: ï¸âš¡ View job at http://192.168.1.130:37107/jfluidity/Graph%20Learning%20%28Diginetica%29/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjY=/version_details/v3
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/1/wandb/run-20240109_012643-31kxe86m/logs
[[36m2024-01-09 02:44:54,357[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - [rank: 0] Metric name is None! Skipping metric value retrieval...[0m
[[36m2024-01-09 02:44:54,360[0m][[35mHYDRA[0m] 	#2 : seed=2 experiment=diginetica_10/gcn[0m
[[36m2024-01-09 02:44:54,712[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - [rank: 0] Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2024-01-09 02:44:54,715[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - [rank: 0] Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.mixed_graph_datamodule.MixedGraphDatamodule          
â”‚       dataset:                                                                
â”‚         _target_: src.data.components.diginetica_dataset.DigineticaDataset    
â”‚         raw_dir: /home/snow/projects/graph-learning/data//Diginetica/processed
â”‚         num_mixture: 10                                                       
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 1400                                                                  
â”‚       - 400                                                                   
â”‚       - 200                                                                   
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.graph_conv_module.GraphConvModule                  
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.encoder.GCNEncoder                    
â”‚         dim_in: 300                                                           
â”‚         dim_hidden: 300                                                       
â”‚         dim_out: 300                                                          
â”‚         num_layers: 3                                                         
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.decoder.NodeMLPDecoder                
â”‚         dim_in: 300                                                           
â”‚         dim_hidden: 300                                                       
â”‚       mapper:                                                                 
â”‚         _target_: src.models.components.mapper.MLPMapper                      
â”‚         dim_in: 300                                                           
â”‚         num_mixture: 10                                                       
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.001                                                             
â”‚         weight_decay: 1.0e-05                                                 
â”‚       num_mixture: 10                                                         
â”‚       scheduler: null                                                         
â”‚       compile: false                                                          
â”‚       learnable_feat: true                                                    
â”‚       num_features: 19519                                                     
â”‚       dim_feature: 300                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/snow/projects/graph-learning/logs/train/multiruns/2024-
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/f1                                                       
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: lightning.pytorch.callbacks.EarlyStopping                   
â”‚         monitor: val/f1                                                       
â”‚         min_delta: 0.0                                                        
â”‚         patience: 10                                                          
â”‚         verbose: false                                                        
â”‚         mode: max                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: lightning.pytorch.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: lightning.pytorch.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ loggerGraph-Learning-Diginetica
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: lightning.pytorch.loggers.wandb.WandbLogger                 
â”‚         save_dir: /home/snow/projects/graph-learning/logs/train/multiruns/2024
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: Graph Learning (Diginetica)                                  
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags:                                                                 
â”‚         - 10-Mixtures                                                         
â”‚         - GCN                                                                 
â”‚         job_type: ''                                                          
â”‚         name: GCN                                                             
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: lightning.pytorch.trainer.Trainer                             
â”‚       default_root_dir: /home/snow/projects/graph-learning/logs/train/multirun
â”‚       min_epochs: 10                                                          
â”‚       max_epochs: 300                                                         
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚       gradient_clip_val: 0                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/snow/projects/graph-learning                            
â”‚       data_dir: /home/snow/projects/graph-learning/data/                      
â”‚       log_dir: /home/snow/projects/graph-learning/logs/                       
â”‚       output_dir: /home/snow/projects/graph-learning/logs/train/multiruns/2024
â”‚       work_dir: /home/snow/projects/graph-learning/src                        
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['10-Mixtures', 'GCN']                                                  
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â””â”€â”€ seed
    â””â”€â”€ 2                                                                       
Seed set to 2
[[36m2024-01-09 02:44:54,792[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating datamodule <src.data.mixed_graph_datamodule.MixedGraphDatamodule>[0m
[[36m2024-01-09 02:45:00,654[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating model <src.models.graph_conv_module.GraphConvModule>[0m
[[36m2024-01-09 02:45:00,744[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating callbacks...[0m
[[36m2024-01-09 02:45:00,744[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2024-01-09 02:45:00,748[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2024-01-09 02:45:00,749[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2024-01-09 02:45:00,750[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2024-01-09 02:45:00,751[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating loggers...[0m
[[36m2024-01-09 02:45:00,751[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>[0m
[[36m2024-01-09 02:45:00,755[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2024-01-09 02:45:00,767[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Logging hyperparameters![0m
[[36m2024-01-09 02:45:02,794[0m][[34murllib3.connectionpool[0m][[33mWARNING[0m] - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLZeroReturnError(6, 'TLS/SSL connection has been closed (EOF) (_ssl.c:1129)'))': /api/4504800232407040/envelope/[0m
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/2/wandb/run-20240109_024500-zzkxmr3x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run GCN
wandb: â­ï¸ View project at http://192.168.1.130:37107/jfluidity/Graph%20Learning%20%28Diginetica%29
wandb: ğŸš€ View run at http://192.168.1.130:37107/jfluidity/Graph%20Learning%20%28Diginetica%29/runs/zzkxmr3x
[[36m2024-01-09 02:45:04,281[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Starting training![0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name             â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder          â”‚ GCNEncoder        â”‚  272 K â”‚
â”‚ 1  â”‚ encoder.conv_in  â”‚ GraphConv         â”‚ 90.3 K â”‚
â”‚ 2  â”‚ encoder.bn_in    â”‚ BatchNorm1d       â”‚    600 â”‚
â”‚ 3  â”‚ encoder.conv_out â”‚ GraphConv         â”‚ 90.3 K â”‚
â”‚ 4  â”‚ encoder.bn_out   â”‚ BatchNorm1d       â”‚    600 â”‚
â”‚ 5  â”‚ encoder.convs    â”‚ ModuleList        â”‚ 90.3 K â”‚
â”‚ 6  â”‚ encoder.convs.0  â”‚ GraphConv         â”‚ 90.3 K â”‚
â”‚ 7  â”‚ encoder.bns      â”‚ ModuleList        â”‚    600 â”‚
â”‚ 8  â”‚ encoder.bns.0    â”‚ BatchNorm1d       â”‚    600 â”‚
â”‚ 9  â”‚ encoder.relu     â”‚ ReLU              â”‚      0 â”‚
â”‚ 10 â”‚ decoder          â”‚ NodeMLPDecoder    â”‚  180 K â”‚
â”‚ 11 â”‚ decoder.linear1  â”‚ Linear            â”‚  180 K â”‚
â”‚ 12 â”‚ decoder.linear2  â”‚ Linear            â”‚    301 â”‚
â”‚ 13 â”‚ decoder.relu     â”‚ ReLU              â”‚      0 â”‚
â”‚ 14 â”‚ mapper           â”‚ MLPMapper         â”‚  5.0 M â”‚
â”‚ 15 â”‚ mapper.fc1       â”‚ Linear            â”‚  451 K â”‚
â”‚ 16 â”‚ mapper.fc2       â”‚ Linear            â”‚  4.5 M â”‚
â”‚ 17 â”‚ mapper.relu      â”‚ ReLU              â”‚      0 â”‚
â”‚ 18 â”‚ train_precision  â”‚ BinaryPrecision   â”‚      0 â”‚
â”‚ 19 â”‚ val_precision    â”‚ BinaryPrecision   â”‚      0 â”‚
â”‚ 20 â”‚ test_precision   â”‚ BinaryPrecision   â”‚      0 â”‚
â”‚ 21 â”‚ train_recall     â”‚ BinaryRecall      â”‚      0 â”‚
â”‚ 22 â”‚ val_recall       â”‚ BinaryRecall      â”‚      0 â”‚
â”‚ 23 â”‚ test_recall      â”‚ BinaryRecall      â”‚      0 â”‚
â”‚ 24 â”‚ train_f1         â”‚ BinaryF1Score     â”‚      0 â”‚
â”‚ 25 â”‚ val_f1           â”‚ BinaryF1Score     â”‚      0 â”‚
â”‚ 26 â”‚ test_f1          â”‚ BinaryF1Score     â”‚      0 â”‚
â”‚ 27 â”‚ train_loss       â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 28 â”‚ val_f1_best      â”‚ MaxMetric         â”‚      0 â”‚
â”‚ 29 â”‚ criterion        â”‚ BCEWithLogitsLoss â”‚      0 â”‚
â”‚ 30 â”‚ sigmoid          â”‚ Sigmoid           â”‚      0 â”‚
â”‚ 31 â”‚ feat             â”‚ Embedding         â”‚  5.9 M â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.3 M                                                        
Non-trainable params: 0                                                         
Total params: 11.3 M                                                            
Total estimated model params size (MB): 45                                      
/home/snow/anaconda3/envs/graph-learning/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
/home/snow/anaconda3/envs/graph-learning/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
/home/snow/anaconda3/envs/graph-learning/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Epoch 12/299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 22/22 0:00:43 â€¢ 0:00:00 0.51it/s v_num: mr3x      
                                                               train/loss_step: 
                                                               0.015            
                                                               val/precision:   
                                                               0.364 val/recall:
                                                               1.000 val/f1:    
                                                               0.533            
                                                               train/loss_epoch:
                                                               0.016            
[[36m2024-01-09 02:56:19,361[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Starting testing![0m
Restoring states from the checkpoint path at /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/2/checkpoints/epoch_002.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
Loaded model weights from the checkpoint at /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/2/checkpoints/epoch_002.ckpt
/home/snow/anaconda3/envs/graph-learning/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚          test/f1          â”‚    0.5430671572685242     â”‚
â”‚      test/precision       â”‚    0.37274685502052307    â”‚
â”‚        test/recall        â”‚            1.0            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 4/4 0:00:03 â€¢ 0:00:00 1.27it/s 
[[36m2024-01-09 02:56:29,841[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Best ckpt path: /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/2/checkpoints/epoch_002.ckpt[0m
[[36m2024-01-09 02:56:29,842[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - [rank: 0] Output dir: /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/2[0m
[[36m2024-01-09 02:56:29,843[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - [rank: 0] Closing wandb![0m
wandb: - 0.047 MB of 0.047 MB uploaded
wandb: \ 0.047 MB of 0.047 MB uploaded
wandb: | 0.123 MB of 0.123 MB uploaded (0.019 MB deduped)
wandb:                                                                                
wandb: W&B sync reduced upload amount by 15.4%             
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb:             test/f1 â–
wandb:      test/precision â–
wandb:         test/recall â–
wandb:    train/loss_epoch â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–
wandb:     train/loss_step â–ˆâ–‡â–ˆâ–ƒâ–
wandb: trainer/global_step â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:              val/f1 â–â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–‡
wandb:       val/precision â–â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–‡
wandb:          val/recall â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 13
wandb:             test/f1 0.54307
wandb:      test/precision 0.37275
wandb:         test/recall 1.0
wandb:    train/loss_epoch 0.01589
wandb:     train/loss_step 0.01519
wandb: trainer/global_step 286
wandb:              val/f1 0.5332
wandb:       val/precision 0.36352
wandb:          val/recall 1.0
wandb: 
wandb: ğŸš€ View run GCN at: http://192.168.1.130:37107/jfluidity/Graph%20Learning%20%28Diginetica%29/runs/zzkxmr3x
wandb: ï¸âš¡ View job at http://192.168.1.130:37107/jfluidity/Graph%20Learning%20%28Diginetica%29/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjY=/version_details/v4
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/2/wandb/run-20240109_024500-zzkxmr3x/logs
[[36m2024-01-09 02:56:35,796[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - [rank: 0] Metric name is None! Skipping metric value retrieval...[0m
[[36m2024-01-09 02:56:35,799[0m][[35mHYDRA[0m] 	#3 : seed=3 experiment=diginetica_10/gcn[0m
[[36m2024-01-09 02:56:36,174[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - [rank: 0] Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2024-01-09 02:56:36,176[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - [rank: 0] Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.mixed_graph_datamodule.MixedGraphDatamodule          
â”‚       dataset:                                                                
â”‚         _target_: src.data.components.diginetica_dataset.DigineticaDataset    
â”‚         raw_dir: /home/snow/projects/graph-learning/data//Diginetica/processed
â”‚         num_mixture: 10                                                       
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 1400                                                                  
â”‚       - 400                                                                   
â”‚       - 200                                                                   
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.graph_conv_module.GraphConvModule                  
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.encoder.GCNEncoder                    
â”‚         dim_in: 300                                                           
â”‚         dim_hidden: 300                                                       
â”‚         dim_out: 300                                                          
â”‚         num_layers: 3                                                         
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.decoder.NodeMLPDecoder                
â”‚         dim_in: 300                                                           
â”‚         dim_hidden: 300                                                       
â”‚       mapper:                                                                 
â”‚         _target_: src.models.components.mapper.MLPMapper                      
â”‚         dim_in: 300                                                           
â”‚         num_mixture: 10                                                       
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.001                                                             
â”‚         weight_decay: 1.0e-05                                                 
â”‚       num_mixture: 10                                                         
â”‚       scheduler: null                                                         
â”‚       compile: false                                                          
â”‚       learnable_feat: true                                                    
â”‚       num_features: 19519                                                     
â”‚       dim_feature: 300                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/snow/projects/graph-learning/logs/train/multiruns/2024-
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/f1                                                       
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: lightning.pytorch.callbacks.EarlyStopping                   
â”‚         monitor: val/f1                                                       
â”‚         min_delta: 0.0                                                        
â”‚         patience: 10                                                          
â”‚         verbose: false                                                        
â”‚         mode: max                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: lightning.pytorch.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progreGraph-Learning-Diginetica                                  
â”‚         _target_: lightning.pytorch.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: lightning.pytorch.loggers.wandb.WandbLogger                 
â”‚         save_dir: /home/snow/projects/graph-learning/logs/train/multiruns/2024
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: Graph Learning (Diginetica)                                  
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags:                                                                 
â”‚         - 10-Mixtures                                                         
â”‚         - GCN                                                                 
â”‚         job_type: ''                                                          
â”‚         name: GCN                                                             
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: lightning.pytorch.trainer.Trainer                             
â”‚       default_root_dir: /home/snow/projects/graph-learning/logs/train/multirun
â”‚       min_epochs: 10                                                          
â”‚       max_epochs: 300                                                         
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚       gradient_clip_val: 0                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/snow/projects/graph-learning                            
â”‚       data_dir: /home/snow/projects/graph-learning/data/                      
â”‚       log_dir: /home/snow/projects/graph-learning/logs/                       
â”‚       output_dir: /home/snow/projects/graph-learning/logs/train/multiruns/2024
â”‚       work_dir: /home/snow/projects/graph-learning/src                        
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['10-Mixtures', 'GCN']                                                  
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â””â”€â”€ seed
    â””â”€â”€ 3                                                                       
Seed set to 3
[[36m2024-01-09 02:56:36,254[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating datamodule <src.data.mixed_graph_datamodule.MixedGraphDatamodule>[0m
[[36m2024-01-09 02:56:42,044[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating model <src.models.graph_conv_module.GraphConvModule>[0m
[[36m2024-01-09 02:56:42,133[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating callbacks...[0m
[[36m2024-01-09 02:56:42,134[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2024-01-09 02:56:42,137[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2024-01-09 02:56:42,138[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2024-01-09 02:56:42,139[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2024-01-09 02:56:42,140[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating loggers...[0m
[[36m2024-01-09 02:56:42,140[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>[0m
[[36m2024-01-09 02:56:42,143[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2024-01-09 02:56:42,156[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Logging hyperparameters![0m
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/3/wandb/run-20240109_025642-e9wpgsku
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run GCN
wandb: â­ï¸ View project at http://192.168.1.130:37107/jfluidity/Graph%20Learning%20%28Diginetica%29
wandb: ğŸš€ View run at http://192.168.1.130:37107/jfluidity/Graph%20Learning%20%28Diginetica%29/runs/e9wpgsku
[[36m2024-01-09 02:56:45,672[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Starting training![0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name             â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder          â”‚ GCNEncoder        â”‚  272 K â”‚
â”‚ 1  â”‚ encoder.conv_in  â”‚ GraphConv         â”‚ 90.3 K â”‚
â”‚ 2  â”‚ encoder.bn_in    â”‚ BatchNorm1d       â”‚    600 â”‚
â”‚ 3  â”‚ encoder.conv_out â”‚ GraphConv         â”‚ 90.3 K â”‚
â”‚ 4  â”‚ encoder.bn_out   â”‚ BatchNorm1d       â”‚    600 â”‚
â”‚ 5  â”‚ encoder.convs    â”‚ ModuleList        â”‚ 90.3 K â”‚
â”‚ 6  â”‚ encoder.convs.0  â”‚ GraphConv         â”‚ 90.3 K â”‚
â”‚ 7  â”‚ encoder.bns      â”‚ ModuleList        â”‚    600 â”‚
â”‚ 8  â”‚ encoder.bns.0    â”‚ BatchNorm1d       â”‚    600 â”‚
â”‚ 9  â”‚ encoder.relu     â”‚ ReLU              â”‚      0 â”‚
â”‚ 10 â”‚ decoder          â”‚ NodeMLPDecoder    â”‚  180 K â”‚
â”‚ 11 â”‚ decoder.linear1  â”‚ Linear            â”‚  180 K â”‚
â”‚ 12 â”‚ decoder.linear2  â”‚ Linear            â”‚    301 â”‚
â”‚ 13 â”‚ decoder.relu     â”‚ ReLU              â”‚      0 â”‚
â”‚ 14 â”‚ mapper           â”‚ MLPMapper         â”‚  5.0 M â”‚
â”‚ 15 â”‚ mapper.fc1       â”‚ Linear            â”‚  451 K â”‚
â”‚ 16 â”‚ mapper.fc2       â”‚ Linear            â”‚  4.5 M â”‚
â”‚ 17 â”‚ mapper.relu      â”‚ ReLU              â”‚      0 â”‚
â”‚ 18 â”‚ train_precision  â”‚ BinaryPrecision   â”‚      0 â”‚
â”‚ 19 â”‚ val_precision    â”‚ BinaryPrecision   â”‚      0 â”‚
â”‚ 20 â”‚ test_precision   â”‚ BinaryPrecision   â”‚      0 â”‚
â”‚ 21 â”‚ train_recall     â”‚ BinaryRecall      â”‚      0 â”‚
â”‚ 22 â”‚ val_recall       â”‚ BinaryRecall      â”‚      0 â”‚
â”‚ 23 â”‚ test_recall      â”‚ BinaryRecall      â”‚      0 â”‚
â”‚ 24 â”‚ train_f1         â”‚ BinaryF1Score     â”‚      0 â”‚
â”‚ 25 â”‚ val_f1           â”‚ BinaryF1Score     â”‚      0 â”‚
â”‚ 26 â”‚ test_f1          â”‚ BinaryF1Score     â”‚      0 â”‚
â”‚ 27 â”‚ train_loss       â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 28 â”‚ val_f1_best      â”‚ MaxMetric         â”‚      0 â”‚
â”‚ 29 â”‚ criterion        â”‚ BCEWithLogitsLoss â”‚      0 â”‚
â”‚ 30 â”‚ sigmoid          â”‚ Sigmoid           â”‚      0 â”‚
â”‚ 31 â”‚ feat             â”‚ Embedding         â”‚  5.9 M â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.3 M                                                        
Non-trainable params: 0                                                         
Total params: 11.3 M                                                            
Total estimated model params size (MB): 45                                      
/home/snow/anaconda3/envs/graph-learning/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
/home/snow/anaconda3/envs/graph-learning/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
/home/snow/anaconda3/envs/graph-learning/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Epoch 12/299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 22/22 0:00:42 â€¢ 0:00:00 0.52it/s v_num: gsku      
                                                               train/loss_step: 
                                                               0.019            
                                                               val/precision:   
                                                               0.378 val/recall:
                                                               1.000 val/f1:    
                                                               0.549            
                                                               train/loss_epoch:
                                                               0.016            
[[36m2024-01-09 03:07:50,111[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Starting testing![0m
Restoring states from the checkpoint path at /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/3/checkpoints/epoch_002.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
Loaded model weights from the checkpoint at /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/3/checkpoints/epoch_002.ckpt
/home/snow/anaconda3/envs/graph-learning/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚          test/f1          â”‚    0.5770634412765503     â”‚
â”‚      test/precision       â”‚    0.4055440425872803     â”‚
â”‚        test/recall        â”‚            1.0            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 4/4 0:00:03 â€¢ 0:00:00 1.31it/s 
[[36m2024-01-09 03:08:00,309[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Best ckpt path: /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/3/checkpoints/epoch_002.ckpt[0m
[[36m2024-01-09 03:08:00,311[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - [rank: 0] Output dir: /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/3[0m
[[36m2024-01-09 03:08:00,311[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - [rank: 0] Closing wandb![0m
wandb: - 0.165 MB of 0.165 MB uploaded (0.019 MB deduped)
wandb:                                                                                
wandb: W&B sync reduced upload amount by 11.5%             
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb:             test/f1 â–
wandb:      test/precision â–
wandb:         test/recall â–
wandb:    train/loss_epoch â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–
wandb:     train/loss_step â–ˆâ–ˆâ–…â–â–ˆ
wandb: trainer/global_step â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:              val/f1 â–ƒâ–‚â–ˆâ–‚â–‚â–â–â–â–‚â–â–â–‚â–ƒ
wandb:       val/precision â–ƒâ–‚â–ˆâ–‚â–‚â–â–â–â–‚â–â–â–‚â–ƒ
wandb:          val/recall â–â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 13
wandb:             test/f1 0.57706
wandb:      test/precision 0.40554
wandb:         test/recall 1.0
wandb:    train/loss_epoch 0.01641
wandb:     train/loss_step 0.01795
wandb: trainer/global_step 286
wandb:              val/f1 0.54879
wandb:       val/precision 0.37816
wandb:          val/recall 1.0
wandb: 
wandb: ğŸš€ View run GCN at: http://192.168.1.130:37107/jfluidity/Graph%20Learning%20%28Diginetica%29/runs/e9wpgsku
wandb: ï¸âš¡ View job at http://192.168.1.130:37107/jfluidity/Graph%20Learning%20%28Diginetica%29/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjY=/version_details/v5
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/3/wandb/run-20240109_025642-e9wpgsku/logs
[[36m2024-01-09 03:08:05,193[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - [rank: 0] Metric name is None! Skipping metric value retrieval...[0m
[[36m2024-01-09 03:08:05,195[0m][[35mHYDRA[0m] 	#4 : seed=4 experiment=diginetica_10/gcn[0m
[[36m2024-01-09 03:08:05,564[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - [rank: 0] Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2024-01-09 03:08:05,566[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - [rank: 0] Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.mixed_graph_datamodule.MixedGraphDatamodule          
â”‚       dataset:                                                                
â”‚         _target_: src.data.components.diginetica_dataset.DigineticaDataset    
â”‚         raw_dir: /home/snow/projects/graph-learning/data//Diginetica/processed
â”‚         num_mixture: 10                                                       
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 1400                                                                  
â”‚       - 400                                                                   
â”‚       - 200                                                                   
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.graph_conv_module.GraphConvModule                  
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.encoder.GCNEncoder                    
â”‚         dim_in: 300                                                           
â”‚         dim_hidden: 300                                                       
â”‚         dim_out: 300                                                          
â”‚         num_layers: 3                                                         
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.decoder.NodeMLPDecoder                
â”‚         dim_in: 300                                                           
â”‚         dim_hidden: 300                                                       
â”‚       mapper:                                                                 
â”‚         _target_: src.models.components.mapper.MLPMapper                      
â”‚         dim_in: 300                                                           
â”‚         num_mixture: 10                                                       
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.001                                                             
â”‚         weight_decay: 1.0e-05                                                 
â”‚       num_mixture: 10                                                         
â”‚       scheduler: null                                                         
â”‚       compile: false                                                          
â”‚       learnable_feat: true                                                    
â”‚       num_features: 19519                                                     
â”‚       dim_feature: 300                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/snow/projects/graph-learning/logs/train/multiruns/2024-
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/f1                                                       
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: lightning.pytorch.callbacks.EarlyStopping                   
â”‚         monitor: val/f1                                                       
â”‚         min_delta: 0.0                                                        
â”‚         patience: 10                                                          
â”‚         verbose: false                                                        
â”‚         mode: max                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: lightning.pytorch.callbacks.RichModelSummary                
â”‚         max_depthGraph-Learning-Diginetica                                  
â”‚       rich_progress_bar:                                                      
â”‚         _target_: lightning.pytorch.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: lightning.pytorch.loggers.wandb.WandbLogger                 
â”‚         save_dir: /home/snow/projects/graph-learning/logs/train/multiruns/2024
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: Graph Learning (Diginetica)                                  
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags:                                                                 
â”‚         - 10-Mixtures                                                         
â”‚         - GCN                                                                 
â”‚         job_type: ''                                                          
â”‚         name: GCN                                                             
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: lightning.pytorch.trainer.Trainer                             
â”‚       default_root_dir: /home/snow/projects/graph-learning/logs/train/multirun
â”‚       min_epochs: 10                                                          
â”‚       max_epochs: 300                                                         
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚       gradient_clip_val: 0                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/snow/projects/graph-learning                            
â”‚       data_dir: /home/snow/projects/graph-learning/data/                      
â”‚       log_dir: /home/snow/projects/graph-learning/logs/                       
â”‚       output_dir: /home/snow/projects/graph-learning/logs/train/multiruns/2024
â”‚       work_dir: /home/snow/projects/graph-learning/src                        
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['10-Mixtures', 'GCN']                                                  
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â””â”€â”€ seed
    â””â”€â”€ 4                                                                       
Seed set to 4
[[36m2024-01-09 03:08:05,643[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating datamodule <src.data.mixed_graph_datamodule.MixedGraphDatamodule>[0m
[[36m2024-01-09 03:08:11,138[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating model <src.models.graph_conv_module.GraphConvModule>[0m
[[36m2024-01-09 03:08:11,225[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating callbacks...[0m
[[36m2024-01-09 03:08:11,226[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2024-01-09 03:08:11,229[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2024-01-09 03:08:11,230[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2024-01-09 03:08:11,231[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2024-01-09 03:08:11,232[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating loggers...[0m
[[36m2024-01-09 03:08:11,232[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - [rank: 0] Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>[0m
[[36m2024-01-09 03:08:11,235[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2024-01-09 03:08:11,246[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Logging hyperparameters![0m
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/4/wandb/run-20240109_030811-wt4m7h5e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run GCN
wandb: â­ï¸ View project at http://192.168.1.130:37107/jfluidity/Graph%20Learning%20%28Diginetica%29
wandb: ğŸš€ View run at http://192.168.1.130:37107/jfluidity/Graph%20Learning%20%28Diginetica%29/runs/wt4m7h5e
[[36m2024-01-09 03:08:14,710[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Starting training![0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name             â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder          â”‚ GCNEncoder        â”‚  272 K â”‚
â”‚ 1  â”‚ encoder.conv_in  â”‚ GraphConv         â”‚ 90.3 K â”‚
â”‚ 2  â”‚ encoder.bn_in    â”‚ BatchNorm1d       â”‚    600 â”‚
â”‚ 3  â”‚ encoder.conv_out â”‚ GraphConv         â”‚ 90.3 K â”‚
â”‚ 4  â”‚ encoder.bn_out   â”‚ BatchNorm1d       â”‚    600 â”‚
â”‚ 5  â”‚ encoder.convs    â”‚ ModuleList        â”‚ 90.3 K â”‚
â”‚ 6  â”‚ encoder.convs.0  â”‚ GraphConv         â”‚ 90.3 K â”‚
â”‚ 7  â”‚ encoder.bns      â”‚ ModuleList        â”‚    600 â”‚
â”‚ 8  â”‚ encoder.bns.0    â”‚ BatchNorm1d       â”‚    600 â”‚
â”‚ 9  â”‚ encoder.relu     â”‚ ReLU              â”‚      0 â”‚
â”‚ 10 â”‚ decoder          â”‚ NodeMLPDecoder    â”‚  180 K â”‚
â”‚ 11 â”‚ decoder.linear1  â”‚ Linear            â”‚  180 K â”‚
â”‚ 12 â”‚ decoder.linear2  â”‚ Linear            â”‚    301 â”‚
â”‚ 13 â”‚ decoder.relu     â”‚ ReLU              â”‚      0 â”‚
â”‚ 14 â”‚ mapper           â”‚ MLPMapper         â”‚  5.0 M â”‚
â”‚ 15 â”‚ mapper.fc1       â”‚ Linear            â”‚  451 K â”‚
â”‚ 16 â”‚ mapper.fc2       â”‚ Linear            â”‚  4.5 M â”‚
â”‚ 17 â”‚ mapper.relu      â”‚ ReLU              â”‚      0 â”‚
â”‚ 18 â”‚ train_precision  â”‚ BinaryPrecision   â”‚      0 â”‚
â”‚ 19 â”‚ val_precision    â”‚ BinaryPrecision   â”‚      0 â”‚
â”‚ 20 â”‚ test_precision   â”‚ BinaryPrecision   â”‚      0 â”‚
â”‚ 21 â”‚ train_recall     â”‚ BinaryRecall      â”‚      0 â”‚
â”‚ 22 â”‚ val_recall       â”‚ BinaryRecall      â”‚      0 â”‚
â”‚ 23 â”‚ test_recall      â”‚ BinaryRecall      â”‚      0 â”‚
â”‚ 24 â”‚ train_f1         â”‚ BinaryF1Score     â”‚      0 â”‚
â”‚ 25 â”‚ val_f1           â”‚ BinaryF1Score     â”‚      0 â”‚
â”‚ 26 â”‚ test_f1          â”‚ BinaryF1Score     â”‚      0 â”‚
â”‚ 27 â”‚ train_loss       â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 28 â”‚ val_f1_best      â”‚ MaxMetric         â”‚      0 â”‚
â”‚ 29 â”‚ criterion        â”‚ BCEWithLogitsLoss â”‚      0 â”‚
â”‚ 30 â”‚ sigmoid          â”‚ Sigmoid           â”‚      0 â”‚
â”‚ 31 â”‚ feat             â”‚ Embedding         â”‚  5.9 M â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.3 M                                                        
Non-trainable params: 0                                                         
Total params: 11.3 M                                                            
Total estimated model params size (MB): 45                                      
/home/snow/anaconda3/envs/graph-learning/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
/home/snow/anaconda3/envs/graph-learning/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
/home/snow/anaconda3/envs/graph-learning/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[[36m2024-01-09 03:51:02,030[0m][[34murllib3.connectionpool[0m][[33mWARNING[0m] - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLZeroReturnError(6, 'TLS/SSL connection has been closed (EOF) (_ssl.c:1129)'))': /api/4504800232407040/envelope/[0m
[[36m2024-01-09 03:51:12,485[0m][[34murllib3.connectionpool[0m][[33mWARNING[0m] - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLZeroReturnError(6, 'TLS/SSL connection has been closed (EOF) (_ssl.c:1129)'))': /api/4504800232407040/envelope/[0m
Epoch 50/299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 22/22 0:00:43 â€¢ 0:00:00 0.51it/s v_num: 7h5e      
                                                               train/loss_step: 
                                                               0.015            
                                                               val/precision:   
                                                               0.377 val/recall:
                                                               1.000 val/f1:    
                                                               0.547            
                                                               train/loss_epoch:
                                                               0.016            
[[36m2024-01-09 03:52:24,139[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Starting testing![0m
Restoring states from the checkpoint path at /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/4/checkpoints/epoch_040.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
Loaded model weights from the checkpoint at /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/4/checkpoints/epoch_040.ckpt
/home/snow/anaconda3/envs/graph-learning/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚          test/f1          â”‚    0.6779565215110779     â”‚
â”‚      test/precision       â”‚    0.5128095746040344     â”‚
â”‚        test/recall        â”‚            1.0            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 4/4 0:00:03 â€¢ 0:00:00 1.29it/s 
[[36m2024-01-09 03:52:34,651[0m][[34m__main__[0m][[32mINFO[0m] - [rank: 0] Best ckpt path: /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/4/checkpoints/epoch_040.ckpt[0m
[[36m2024-01-09 03:52:34,653[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - [rank: 0] Output dir: /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/4[0m
[[36m2024-01-09 03:52:34,654[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - [rank: 0] Closing wandb![0m
wandb: - 0.207 MB of 0.207 MB uploaded (0.019 MB deduped)
wandb:                                                                                
wandb: W&B sync reduced upload amount by 9.1%             
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:             test/f1 â–
wandb:      test/precision â–
wandb:         test/recall â–
wandb:    train/loss_epoch â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–ˆâ–‚â–â–â–â–â–
wandb:     train/loss_step â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:              val/f1 â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ƒâ–†â–‡â–†â–†â–†â–†
wandb:       val/precision â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–‚â–†â–†â–†â–†â–†â–†
wandb:          val/recall â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 51
wandb:             test/f1 0.67796
wandb:      test/precision 0.51281
wandb:         test/recall 1.0
wandb:    train/loss_epoch 0.01604
wandb:     train/loss_step 0.01524
wandb: trainer/global_step 1122
wandb:              val/f1 0.54725
wandb:       val/precision 0.37671
wandb:          val/recall 0.99998
wandb: 
wandb: ğŸš€ View run GCN at: http://192.168.1.130:37107/jfluidity/Graph%20Learning%20%28Diginetica%29/runs/wt4m7h5e
wandb: ï¸âš¡ View job at http://192.168.1.130:37107/jfluidity/Graph%20Learning%20%28Diginetica%29/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjY=/version_details/v6
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: /home/snow/projects/graph-learning/logs/train/multiruns/2024-01-09_01-11-41/4/wandb/run-20240109_030811-wt4m7h5e/logs
[[36m2024-01-09 03:52:40,418[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - [rank: 0] Metric name is None! Skipping metric value retrieval...[0m
